import numpy as np
from sklearn.base import BaseEstimator
from scipy.special import expit
from time import time


class LogReg(BaseEstimator):
    def __init__(self, lambda_1=0.0, lambda_2=1.0, gd_type='stochastic',
                 tolerance=1e-4, max_iter=1000, w0=None, alpha=1e-3):
        """
        lambda_1: L1 regularization param
        lambda_2: L2 regularization param
        gd_type: 'full' or 'stochastic'
        tolerance: for stopping gradient descent
        max_iter: maximum number of steps in gradient descent
        w0: np.array of shape (d) - init weights
        alpha: learning rate
        """
        self.lambda_1 = lambda_1
        self.lambda_2 = lambda_2
        self.gd_type = gd_type
        self.tolerance = tolerance
        self.max_iter = max_iter
        self.w0 = w0
        self.alpha = alpha
        self.w = None
        self.loss_history = None

    def fit(self, X, y):
        """
        X: np.array of shape (l, d)
        y: np.array of shape (l)
        ---
        output: self
        """
        self.loss_history = []
        l = X.shape[0]
        d = X.shape[1]
        if self.w0 is None:
            self.w0 = np.zeros(l)
        prev_w = np.ones(l)
        self.w = w0
        it = 0
        currenttime = 0.
        while it < self.max_iter and np.norm(prev_w - self.w) >= self.tolerance:
            tmptime = time()
            it += 1
            prev_w = np.copy(self.w)
            if self.gd_type == 'full':
                self.w = self.w - self.alpha * self.calc_gradient(X, y)
            else:
                index = np.random.randint(0, l)
                grad = self.calc_gradient(X[index].reshape((1, d)), y)
                self.w = self.w - self.alpha * grad
            currenttime += time() - tmptime
            self.loss_history.append((self.calc_loss(X, y), currenttime))
        return self

    def predict_proba(self, X):
        """
        X: np.array of shape (l, d)
        ---
        output: np.array of shape (l, 2) where
        first column has probabilities of -1
        second column has probabilities of +1
        """
        if self.w is None:
            raise Exception('Not trained yet')

        res = (1 / (1 + np.exp(-np.dot(X, self.w)))).reshape((X.shape[0], 1))
        return np.hstack((1 - res, res))

    def calc_gradient(self, X, y):
        """
        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)
        y: np.array of shape (l)
        ---
        output: np.array of shape (d)
        """
        tmp = y * expit(-y * np.dot(X, self.w))
        grad = -(X * tmp[:, np.newaxis]) + (self.lambda_2 * self.w)[np.newaxis, :]
        return np.sum(grad, axis=0) / X.shape[0]

    def calc_loss(self, X, y):
        """
        X: np.array of shape (l, d)
        y: np.array of shape (l)
        ---
        output: float
        """

        l = X.shape[0]
        tmp = -np.log(expit(y * np.dot(X, self.w)))
        return np.sum(tmp) / l + self.lambda_2 * np.sum(self.w ** 2) / 2
